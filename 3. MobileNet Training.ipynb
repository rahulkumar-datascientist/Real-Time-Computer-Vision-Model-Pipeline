{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVyHcUFcsdbH"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "o38-RUuhsDyB"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from keras.applications import MobileNetV2\n",
    "from keras.applications.mobilenet_v2 import preprocess_input\n",
    "import numpy as np\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baEBjTQjm8yR"
   },
   "source": [
    "## Retraining MobileNet for generating custom model via Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddJoWvjy58QT"
   },
   "source": [
    "## Non-optimised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXz9kufhmgN7"
   },
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gmlNMFYm8_CU",
    "outputId": "0677ffd6-4a80-4e4c-b18e-e97d22d916dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 736 images belonging to 2 classes.\n",
      "Found 184 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data fetch\n",
    "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                 validation_split=0.2)\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory(\"./scrapped images\",\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=10,\n",
    "                                                 class_mode='binary',\n",
    "                                                 shuffle=True,\n",
    "                                                 subset='training')\n",
    "\n",
    "validation_generator=train_datagen.flow_from_directory(\"./scrapped images\",\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=10,\n",
    "                                                 class_mode='binary',\n",
    "                                                 shuffle=True,\n",
    "                                                 subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00EvR4Mamnk6"
   },
   "source": [
    "### Defining Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zluWFfEzjVf-",
    "outputId": "aacc841d-d817-49f9-ea11-8c0cd98bd99d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 0s 0us/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "mobilenetv2_1.00_224 (Functi (None, 7, 7, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 1281      \n",
      "=================================================================\n",
      "Total params: 2,259,265\n",
      "Trainable params: 1,281\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Loading the MobileNet model\n",
    "base_model = MobileNetV2(\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    ") \n",
    "\n",
    "# Freeze the base_model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create new model on top\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "\n",
    "# The base model contains batchnorm layers. We want to keep them in inference \n",
    "# mode when we unfreeze the base model for fine-tuning, so we make sure that the\n",
    "# base_model is running in inference mode here.\n",
    "x = base_model(inputs, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "outputs = keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4aPBGLAE3O8"
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gff3n1hgjVgR",
    "outputId": "bf8c9027-85e5-47aa-a6da-f7a812dd06fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "23/23 [==============================] - 305s 12s/step - loss: 0.8362 - binary_accuracy: 0.5068 - val_loss: 0.7355 - val_binary_accuracy: 0.5375\n",
      "Epoch 2/25\n",
      "23/23 [==============================] - 5s 215ms/step - loss: 0.7371 - binary_accuracy: 0.5145 - val_loss: 0.6949 - val_binary_accuracy: 0.5500\n",
      "Epoch 3/25\n",
      "23/23 [==============================] - 5s 217ms/step - loss: 0.7138 - binary_accuracy: 0.5545 - val_loss: 0.6684 - val_binary_accuracy: 0.5813\n",
      "Epoch 4/25\n",
      "23/23 [==============================] - 5s 218ms/step - loss: 0.6747 - binary_accuracy: 0.5955 - val_loss: 0.6469 - val_binary_accuracy: 0.6500\n",
      "Epoch 5/25\n",
      "23/23 [==============================] - 5s 218ms/step - loss: 0.6409 - binary_accuracy: 0.6046 - val_loss: 0.6228 - val_binary_accuracy: 0.6812\n",
      "Epoch 6/25\n",
      "23/23 [==============================] - 5s 215ms/step - loss: 0.6045 - binary_accuracy: 0.6951 - val_loss: 0.6132 - val_binary_accuracy: 0.7188\n",
      "Epoch 7/25\n",
      "23/23 [==============================] - 5s 217ms/step - loss: 0.5646 - binary_accuracy: 0.7150 - val_loss: 0.5758 - val_binary_accuracy: 0.7750\n",
      "Epoch 8/25\n",
      "23/23 [==============================] - 5s 216ms/step - loss: 0.5551 - binary_accuracy: 0.7019 - val_loss: 0.5637 - val_binary_accuracy: 0.7750\n",
      "Epoch 9/25\n",
      "23/23 [==============================] - 5s 216ms/step - loss: 0.5436 - binary_accuracy: 0.7439 - val_loss: 0.5622 - val_binary_accuracy: 0.7812\n",
      "Epoch 10/25\n",
      "23/23 [==============================] - 5s 216ms/step - loss: 0.5309 - binary_accuracy: 0.7500 - val_loss: 0.5352 - val_binary_accuracy: 0.8062\n",
      "Epoch 11/25\n",
      "23/23 [==============================] - 5s 220ms/step - loss: 0.5207 - binary_accuracy: 0.7789 - val_loss: 0.5204 - val_binary_accuracy: 0.8188\n",
      "Epoch 12/25\n",
      "23/23 [==============================] - 5s 218ms/step - loss: 0.4835 - binary_accuracy: 0.8005 - val_loss: 0.5117 - val_binary_accuracy: 0.8250\n",
      "Epoch 13/25\n",
      "23/23 [==============================] - 5s 218ms/step - loss: 0.4781 - binary_accuracy: 0.8010 - val_loss: 0.4966 - val_binary_accuracy: 0.8188\n",
      "Epoch 14/25\n",
      "23/23 [==============================] - 5s 217ms/step - loss: 0.4700 - binary_accuracy: 0.8064 - val_loss: 0.4882 - val_binary_accuracy: 0.8313\n",
      "Epoch 15/25\n",
      "23/23 [==============================] - 5s 217ms/step - loss: 0.4544 - binary_accuracy: 0.8167 - val_loss: 0.4813 - val_binary_accuracy: 0.8375\n",
      "Epoch 16/25\n",
      "23/23 [==============================] - 5s 217ms/step - loss: 0.4696 - binary_accuracy: 0.7877 - val_loss: 0.4623 - val_binary_accuracy: 0.8375\n",
      "Epoch 17/25\n",
      "23/23 [==============================] - 5s 220ms/step - loss: 0.4580 - binary_accuracy: 0.8157 - val_loss: 0.4505 - val_binary_accuracy: 0.8313\n",
      "Epoch 18/25\n",
      "23/23 [==============================] - 5s 218ms/step - loss: 0.4287 - binary_accuracy: 0.8176 - val_loss: 0.4578 - val_binary_accuracy: 0.8250\n",
      "Epoch 19/25\n",
      "23/23 [==============================] - 5s 216ms/step - loss: 0.4129 - binary_accuracy: 0.8216 - val_loss: 0.4540 - val_binary_accuracy: 0.8438\n",
      "Epoch 20/25\n",
      "23/23 [==============================] - 5s 220ms/step - loss: 0.4070 - binary_accuracy: 0.8365 - val_loss: 0.4310 - val_binary_accuracy: 0.8438\n",
      "Epoch 21/25\n",
      "23/23 [==============================] - 5s 215ms/step - loss: 0.3878 - binary_accuracy: 0.8531 - val_loss: 0.4275 - val_binary_accuracy: 0.8500\n",
      "Epoch 22/25\n",
      "23/23 [==============================] - 5s 215ms/step - loss: 0.3745 - binary_accuracy: 0.8863 - val_loss: 0.4030 - val_binary_accuracy: 0.8625\n",
      "Epoch 23/25\n",
      "23/23 [==============================] - 5s 228ms/step - loss: 0.3786 - binary_accuracy: 0.8569 - val_loss: 0.4121 - val_binary_accuracy: 0.8687\n",
      "Epoch 24/25\n",
      "23/23 [==============================] - 5s 223ms/step - loss: 0.3598 - binary_accuracy: 0.8764 - val_loss: 0.4250 - val_binary_accuracy: 0.8438\n",
      "Epoch 25/25\n",
      "23/23 [==============================] - 5s 219ms/step - loss: 0.3569 - binary_accuracy: 0.8776 - val_loss: 0.4151 - val_binary_accuracy: 0.8313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feb40c0d690>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#last layers training\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-4),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()]\n",
    ")\n",
    "\n",
    "epochs = 25\n",
    "\n",
    "model.fit(x=train_generator,\n",
    "          steps_per_epoch = train_generator.n//train_generator.batch_size,\n",
    "          validation_data = validation_generator, \n",
    "          validation_steps = validation_generator.n//validation_generator.batch_size,\n",
    "          epochs = epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqGU-gXejtof"
   },
   "source": [
    "### Model Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g8KTxa0dh_Qg",
    "outputId": "96639b3a-74b9-40a5-afbf-6a9d1ff806c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "mobilenetv2_1.00_224 (Functi (None, 7, 7, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 1281      \n",
      "=================================================================\n",
      "Total params: 2,259,265\n",
      "Trainable params: 2,225,153\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 11s 315ms/step - loss: 0.3520 - binary_accuracy: 0.8651 - val_loss: 0.3910 - val_binary_accuracy: 0.8500\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 6s 278ms/step - loss: 0.3254 - binary_accuracy: 0.8974 - val_loss: 0.3704 - val_binary_accuracy: 0.8438\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 7s 277ms/step - loss: 0.3254 - binary_accuracy: 0.8866 - val_loss: 0.3714 - val_binary_accuracy: 0.8562\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 7s 280ms/step - loss: 0.3224 - binary_accuracy: 0.8714 - val_loss: 0.3614 - val_binary_accuracy: 0.8750\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 6s 276ms/step - loss: 0.3130 - binary_accuracy: 0.8806 - val_loss: 0.3650 - val_binary_accuracy: 0.8438\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 6s 269ms/step - loss: 0.2779 - binary_accuracy: 0.9029 - val_loss: 0.3430 - val_binary_accuracy: 0.8500\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 6s 275ms/step - loss: 0.2872 - binary_accuracy: 0.8878 - val_loss: 0.3569 - val_binary_accuracy: 0.8500\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 6s 274ms/step - loss: 0.2532 - binary_accuracy: 0.9051 - val_loss: 0.3434 - val_binary_accuracy: 0.8500\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 7s 279ms/step - loss: 0.2673 - binary_accuracy: 0.9091 - val_loss: 0.3250 - val_binary_accuracy: 0.8687\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 7s 286ms/step - loss: 0.2545 - binary_accuracy: 0.8996 - val_loss: 0.3086 - val_binary_accuracy: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feb4017bc50>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine tuning\n",
    "base_model.trainable = True\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-6),  # Low learning rate\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()]\n",
    ")\n",
    "\n",
    "epochs = 10\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch = train_generator.n//train_generator.batch_size,\n",
    "                    validation_data = validation_generator, \n",
    "                    validation_steps = validation_generator.n//validation_generator.batch_size,\n",
    "                    epochs = epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9MKkwBhj1x9"
   },
   "source": [
    "### Saving the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62uL6L0vdT8b"
   },
   "outputs": [],
   "source": [
    "model.save(\"Final_modelN_89_87.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AUMu5qj7Qr8"
   },
   "source": [
    "## Optimised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcUGeNfykZgx"
   },
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4prZ_X4FGrTR",
    "outputId": "33893f72-e75e-4f84-c8e5-f40bf19c02fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 736 images belonging to 2 classes.\n",
      "Found 184 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data fetch\n",
    "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                 rotation_range=10,                                       # Data Agumentations\n",
    "                                 zoom_range=0.15,\n",
    "                                 height_shift_range=0.5,\n",
    "                                 horizontal_flip=True,\n",
    "                                 validation_split=0.2)\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory(\"./scrapped images\",\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='binary',\n",
    "                                                 shuffle=True,\n",
    "                                                 subset='training')\n",
    "\n",
    "validation_generator=train_datagen.flow_from_directory(\"./scrapped images\",\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='binary',\n",
    "                                                 shuffle=True,\n",
    "                                                 subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96OGBbn3GrT1"
   },
   "source": [
    "### Defining Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AGqKHJcFR_UK",
    "outputId": "2357f54c-5d3f-4c8f-e0bf-619034ebab83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "mobilenetv2_1.00_224 (Functi (None, 7, 7, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1281      \n",
      "=================================================================\n",
      "Total params: 2,259,265\n",
      "Trainable params: 1,281\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Loading the MobileNet model\n",
    "base_model = MobileNetV2(\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    ") \n",
    "\n",
    "# Freeze the base_model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create new model on top\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "\n",
    "# The base model contains batchnorm layers. We want to keep them in inference \n",
    "# mode when we unfreeze the base model for fine-tuning, so we make sure that the\n",
    "# base_model is running in inference mode here.\n",
    "x = base_model(inputs, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "outputs = keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyRhelR3kZg7"
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffp5gvCbR_UL",
    "outputId": "c6c35177-64e0-45c4-a564-a1602d6a3d3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "23/23 [==============================] - 167s 7s/step - loss: 0.8117 - binary_accuracy: 0.5545 - val_loss: 0.7418 - val_binary_accuracy: 0.5125\n",
      "Epoch 2/25\n",
      "23/23 [==============================] - 14s 608ms/step - loss: 0.7212 - binary_accuracy: 0.5715 - val_loss: 0.7093 - val_binary_accuracy: 0.5813\n",
      "Epoch 3/25\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 0.7484 - binary_accuracy: 0.5393 - val_loss: 0.7111 - val_binary_accuracy: 0.5688\n",
      "Epoch 4/25\n",
      "23/23 [==============================] - 14s 615ms/step - loss: 0.6817 - binary_accuracy: 0.5677 - val_loss: 0.6789 - val_binary_accuracy: 0.5688\n",
      "Epoch 5/25\n",
      "23/23 [==============================] - 14s 608ms/step - loss: 0.6534 - binary_accuracy: 0.6290 - val_loss: 0.6722 - val_binary_accuracy: 0.5688\n",
      "Epoch 6/25\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 0.6587 - binary_accuracy: 0.6312 - val_loss: 0.6457 - val_binary_accuracy: 0.6250\n",
      "Epoch 7/25\n",
      "23/23 [==============================] - 14s 608ms/step - loss: 0.6297 - binary_accuracy: 0.6760 - val_loss: 0.6621 - val_binary_accuracy: 0.6375\n",
      "Epoch 8/25\n",
      "23/23 [==============================] - 14s 601ms/step - loss: 0.5891 - binary_accuracy: 0.7053 - val_loss: 0.6000 - val_binary_accuracy: 0.6938\n",
      "Epoch 9/25\n",
      "23/23 [==============================] - 14s 612ms/step - loss: 0.6027 - binary_accuracy: 0.6794 - val_loss: 0.5971 - val_binary_accuracy: 0.6812\n",
      "Epoch 10/25\n",
      "23/23 [==============================] - 14s 613ms/step - loss: 0.5795 - binary_accuracy: 0.6906 - val_loss: 0.6577 - val_binary_accuracy: 0.6000\n",
      "Epoch 11/25\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 0.5619 - binary_accuracy: 0.7117 - val_loss: 0.5975 - val_binary_accuracy: 0.6812\n",
      "Epoch 12/25\n",
      "23/23 [==============================] - 14s 604ms/step - loss: 0.5742 - binary_accuracy: 0.6999 - val_loss: 0.5768 - val_binary_accuracy: 0.6750\n",
      "Epoch 13/25\n",
      "23/23 [==============================] - 14s 595ms/step - loss: 0.5404 - binary_accuracy: 0.7333 - val_loss: 0.5901 - val_binary_accuracy: 0.7000\n",
      "Epoch 14/25\n",
      "23/23 [==============================] - 14s 606ms/step - loss: 0.5390 - binary_accuracy: 0.7109 - val_loss: 0.5738 - val_binary_accuracy: 0.6625\n",
      "Epoch 15/25\n",
      "23/23 [==============================] - 14s 606ms/step - loss: 0.5720 - binary_accuracy: 0.7015 - val_loss: 0.5951 - val_binary_accuracy: 0.6625\n",
      "Epoch 16/25\n",
      "23/23 [==============================] - 14s 602ms/step - loss: 0.5273 - binary_accuracy: 0.7274 - val_loss: 0.5500 - val_binary_accuracy: 0.7063\n",
      "Epoch 17/25\n",
      "23/23 [==============================] - 14s 605ms/step - loss: 0.5273 - binary_accuracy: 0.7458 - val_loss: 0.5403 - val_binary_accuracy: 0.7312\n",
      "Epoch 18/25\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 0.5140 - binary_accuracy: 0.7500 - val_loss: 0.5187 - val_binary_accuracy: 0.7625\n",
      "Epoch 19/25\n",
      "23/23 [==============================] - 14s 604ms/step - loss: 0.4904 - binary_accuracy: 0.7755 - val_loss: 0.5417 - val_binary_accuracy: 0.7500\n",
      "Epoch 20/25\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 0.5002 - binary_accuracy: 0.7572 - val_loss: 0.5679 - val_binary_accuracy: 0.7063\n",
      "Epoch 21/25\n",
      "23/23 [==============================] - 14s 596ms/step - loss: 0.5156 - binary_accuracy: 0.7624 - val_loss: 0.4978 - val_binary_accuracy: 0.7688\n",
      "Epoch 22/25\n",
      "23/23 [==============================] - 14s 610ms/step - loss: 0.4688 - binary_accuracy: 0.7777 - val_loss: 0.5217 - val_binary_accuracy: 0.7563\n",
      "Epoch 23/25\n",
      "23/23 [==============================] - 14s 591ms/step - loss: 0.4607 - binary_accuracy: 0.8172 - val_loss: 0.5018 - val_binary_accuracy: 0.7375\n",
      "Epoch 24/25\n",
      "23/23 [==============================] - 14s 611ms/step - loss: 0.4820 - binary_accuracy: 0.7752 - val_loss: 0.5098 - val_binary_accuracy: 0.8062\n",
      "Epoch 25/25\n",
      "23/23 [==============================] - 14s 603ms/step - loss: 0.4609 - binary_accuracy: 0.8059 - val_loss: 0.4649 - val_binary_accuracy: 0.8062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1f5a108890>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#last layers training\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-4),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()]\n",
    ")\n",
    "\n",
    "epochs = 25\n",
    "\n",
    "reduce = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, mode='auto')\n",
    "early = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=6, mode='auto')\n",
    "filepath = \"./Retrained MobileNet Models/training-model-{epoch:02d}.h5\"\n",
    "model_save = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', save_freq=5)\n",
    "\n",
    "model.fit(x=train_generator,\n",
    "          steps_per_epoch = train_generator.n//train_generator.batch_size,\n",
    "          validation_data = validation_generator, \n",
    "          validation_steps = validation_generator.n//validation_generator.batch_size,\n",
    "          epochs = epochs,\n",
    "          callbacks=[reduce, early, model_save])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rY10qt0FmuM6"
   },
   "source": [
    "### Model Fine Tuning\n",
    "By training all layers a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VbYEnEJDR_UN",
    "outputId": "72be3f9e-b531-4c43-9f56-c892f7b99bc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "mobilenetv2_1.00_224 (Functi (None, 7, 7, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1281      \n",
      "=================================================================\n",
      "Total params: 2,259,265\n",
      "Trainable params: 2,225,153\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 33s 1s/step - loss: 0.4661 - binary_accuracy: 0.7805 - val_loss: 0.4882 - val_binary_accuracy: 0.7750\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 28s 1s/step - loss: 0.4198 - binary_accuracy: 0.8235 - val_loss: 0.4811 - val_binary_accuracy: 0.7875\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 28s 1s/step - loss: 0.4734 - binary_accuracy: 0.7855 - val_loss: 0.4546 - val_binary_accuracy: 0.7812\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 27s 1s/step - loss: 0.4175 - binary_accuracy: 0.8109 - val_loss: 0.4583 - val_binary_accuracy: 0.7688\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 28s 1s/step - loss: 0.4065 - binary_accuracy: 0.8376 - val_loss: 0.4444 - val_binary_accuracy: 0.8000\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 29s 1s/step - loss: 0.4353 - binary_accuracy: 0.7957 - val_loss: 0.4326 - val_binary_accuracy: 0.8062\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 27s 1s/step - loss: 0.4019 - binary_accuracy: 0.8116 - val_loss: 0.3757 - val_binary_accuracy: 0.8562\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 28s 1s/step - loss: 0.4324 - binary_accuracy: 0.7983 - val_loss: 0.4014 - val_binary_accuracy: 0.7812\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 28s 1s/step - loss: 0.3864 - binary_accuracy: 0.8242 - val_loss: 0.4033 - val_binary_accuracy: 0.8125\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 28s 1s/step - loss: 0.3923 - binary_accuracy: 0.8190 - val_loss: 0.3714 - val_binary_accuracy: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1f4cf9ecd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine tuning\n",
    "base_model.trainable = True\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-6),  # Low learning rate\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()]\n",
    ")\n",
    "\n",
    "reduce = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, mode='auto')\n",
    "early = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=4, mode='auto')\n",
    "filepath = \"./Retrained MobileNet Models/Final-model-{epoch:02d}.h5\"\n",
    "model_save = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', save_freq=1)\n",
    "\n",
    "epochs = 10\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch = train_generator.n//train_generator.batch_size,\n",
    "                    validation_data = validation_generator, \n",
    "                    validation_steps = validation_generator.n//validation_generator.batch_size,\n",
    "                    epochs = epochs,\n",
    "                    callbacks=[reduce, early, model_save])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMKwsbMhG3LN"
   },
   "source": [
    "Due to use of Model checkpoint callback, we saved the models after every epoch. Considering the model after 10th epoch with training accuracy of 81.9 and validation accuracy of 87.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13hYCRkxLBN9"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXg1l3Uhl3U1"
   },
   "source": [
    "### Load both the models for predecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fNVuqTAkkZhC"
   },
   "outputs": [],
   "source": [
    "nonOpti_model = load_model('./Retrained MobileNet Models/Final_modelN_89_87.h5')\n",
    "Opti_model    = load_model('./Retrained MobileNet Models/Final-model-10.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6xKmpQ-nOYo"
   },
   "source": [
    "**Function required for prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qyCWy9kFkoce"
   },
   "outputs": [],
   "source": [
    "def load_image(img_path, show=False):\n",
    "\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_tensor = image.img_to_array(img)                    # (height, width, channels)\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=0)         # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels)\n",
    "    img_tensor /= 255.                                      # imshow expects values in the range [0, 1]\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(img_tensor[0])                           \n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qyync3wzlHV4"
   },
   "source": [
    "**From the code below, directly model.predict() will be called on the \"bounding box of a car\".**   \n",
    "The results could be interpreted as\n",
    "* if \"<0.5\" then Sedan\n",
    "* if \">0.5\" then SUV\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xl2_L_rAE13d",
    "outputId": "afce602d-e0c8-4704-aa89-bc137fce026c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-opti\n",
      "\n",
      "s1: [[0.7645307]]\n",
      "s2: [[0.9616349]]\n",
      "s3: [[0.5487058]]\n",
      "s4: [[0.9736796]]\n",
      "s5: [[0.887488]]\n",
      "s6: [[0.93356997]]\n",
      "s7: [[0.9238109]]\n",
      "se1: [[0.11395901]]\n",
      "se2: [[0.04612371]]\n",
      "se3: [[0.25387716]]\n",
      "se4: [[0.11364701]]\n",
      "\n",
      "\n",
      "Opti\n",
      "\n",
      "s1: [[0.71382827]]\n",
      "s2: [[0.94301295]]\n",
      "s3: [[0.7187693]]\n",
      "s4: [[0.9572166]]\n",
      "s5: [[0.6194374]]\n",
      "s6: [[0.9540255]]\n",
      "s7: [[0.92153907]]\n",
      "se1: [[0.14523795]]\n",
      "se2: [[0.09738412]]\n",
      "se3: [[0.28262565]]\n",
      "se4: [[0.1089395]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# img_path = '/content/gdrive/My Drive/Colab_Notebooks/Case_Studies/MobileNet/cars_test_mini/s1.jpg'\n",
    "# new_image = load_image(img_path)\n",
    "# pred = model.predict(new_image)\n",
    "# print(pred)\n",
    "\n",
    "print(\"Non-opti\\n\")\n",
    "for i in range(1,8):\n",
    "  img_path = './Archive/Test images/s'+str(i)+'.jpg'\n",
    "  new_image = load_image(img_path)\n",
    "  pred = nonOpti_model.predict(new_image)\n",
    "  #pred1 = decode_predictions(pred)  \n",
    "  print(\"s\"+str(i)+\": \"+str(pred))\n",
    "\n",
    "for i in range(1,5):\n",
    "  img_path = './Archive/Test images/se'+str(i)+'.jpg'\n",
    "  new_image = load_image(img_path)\n",
    "  pred = nonOpti_model.predict(new_image)\n",
    "  #pred1 = decode_predictions(pred)\n",
    "  print(\"se\"+str(i)+\": \"+str(pred))  \n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Opti\\n\")\n",
    "for i in range(1,8):\n",
    "  img_path = './Archive/Test images/s'+str(i)+'.jpg'\n",
    "  new_image = load_image(img_path)\n",
    "  pred = Opti_model.predict(new_image)\n",
    "  #pred1 = decode_predictions(pred)  \n",
    "  print(\"s\"+str(i)+\": \"+str(pred))\n",
    "\n",
    "for i in range(1,5):\n",
    "  img_path = './Archive/Test images/se'+str(i)+'.jpg'\n",
    "  new_image = load_image(img_path)\n",
    "  pred = Opti_model.predict(new_image)\n",
    "  #pred1 = decode_predictions(pred)\n",
    "  print(\"se\"+str(i)+\": \"+str(pred))    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNquZR3vagmwXZZPNt7FWzE",
   "collapsed_sections": [],
   "mount_file_id": "1-9nMU4AIn_rDjNAW2G3QTSKYiT36SkUp",
   "name": "3. MobileNet Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
